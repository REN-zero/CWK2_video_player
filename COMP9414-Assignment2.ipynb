{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHWjhahSrrjw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/UNSW-COMP9414/Assignment2/blob/main/COMP9414-Assignment2.ipynb)\n",
        "\n",
        "# COMP9414 24T3 - Assignment 2 - Neural Networks, Decision Trees and Random Forests\n",
        "\n",
        "## UNSW Sydney\n",
        "\n",
        "Designed by Gustavo Batista.\n",
        "\n",
        "Last change: 20th October, 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY9vH1EVrrjz"
      },
      "source": [
        "Student name - zID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RATElIHUrrjz"
      },
      "source": [
        "## Instructions\n",
        "\n",
        "**Submission deadline:** Friday, 8th November 2024, at 17:00:00 AEDT.\n",
        "\n",
        "**Submission:** You can submit your solution via the give system using the command ``give cs9414 ass2 ass2.ipynb``.\n",
        "\n",
        "**Instructions:**\n",
        "* This is an **individual** assignment.\n",
        "* Write your name and zID on the top of this Jupyter Notebook.\n",
        "* You can only use the libraries listed in this notebook\n",
        "* You can reuse any piece of source code developed in the tutorials.\n",
        "* Do not modify the existing code in this notebook except to answer the questions. The cells that should be modified are indicated.\n",
        "* If you want to submit additional code (e.g., for generating plots), write it at the end of the notebook.\n",
        "* This notebook is worth **75** marks and will be rescaled to **25** marks.\n",
        "\n",
        "**Late Submission Policy:** A 5% reduction of the assignment value (i.e. 1.25 marks) will be applied per day for late submissions. For example, if an assignment gets an on-time mark of $20/25$ but is submitted three days late, the penalty will be $3*1.25 = 3.75$, so the final mark will be $16.25$. After five days, the assignment total mark will be reduced to 0 ($100\\%$ reduction). An assignment is considered one day late if submitted any time after the submission deadline, up to 24 hours past it.\n",
        "\n",
        "**Plagiarism:**\n",
        "\n",
        "Remember that ALL work submitted for this assignment must be your own work, and no sharing or copying of code or answers is allowed. You may discuss the assignment with other students but must not collaborate to develop answers to the questions. You may use code from the Internet only with suitable attribution of the source. You may not use ChatGPT or any similar software to generate any part of your explanations, evaluations or code. Do not use public code repositories on sites such as GitHub or file-sharing sites such as Google Drive to save any part of your work &ndash; make sure your code repository or cloud storage is private, and do not share any links. This also applies after you have finished the course, as we do not want next year’s students accessing your solution, and plagiarism penalties can still apply after the course has finished.\n",
        "\n",
        "All submitted assignments will be run through plagiarism detection software to detect similarities to other submissions, including from past years. You should **carefully** read the UNSW policy on academic integrity and plagiarism (linked from the course web page), noting, in particular, that collusion (working together on an assignment or sharing parts of assignment solutions) is a form of plagiarism.\n",
        "\n",
        "Finally, do not use any contract cheating “academies” or online “tutoring” services. This counts as serious misconduct with heavy penalties up to automatic failure of the course with 0 marks and expulsion from the university for repeat offenders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SDqVhjrrjz"
      },
      "source": [
        "## Technical prerequisites\n",
        "\n",
        "These are the libraries you are allowed to use. No other libraries will be accepted. Make sure you are using Python 3."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64FMmpdjrs5_",
        "outputId": "1abaf07d-3765-4fe2-f33a-9e340224f736"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.13.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rJPmxDqArrj0"
      },
      "outputs": [],
      "source": [
        "# These are the allowed libraries. You can add other libraries used in the tutorials.\n",
        "\n",
        "# Common Python libraries\n",
        "import math\n",
        "import copy\n",
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "import matplotlib as mp\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Scikit-Learn libraries for data preprocessing and model assessment\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Libraries for the tree models\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "\n",
        "# Scikit-learn libraries for hyperparameter tuning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Tensorflow/keras libraries for shallow and deep-learning models\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "# Keras Tuner libraries for hyperparameter tuning\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "# Libraries to present results in tabular format\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQp9lqyFrrj1"
      },
      "source": [
        "This assignment compares three Machine Learning approaches: Neural Networks, Decision Trees, and Random Forests. We will assess these approaches in five benchmark datasets with diverse characteristics.\n",
        "\n",
        "We would like to test a few hypotheses based on common Machine Learning wisdom and misconceptions.\n",
        "\n",
        "1. Neural networks are the best general classifiers regarding prediction quality (accuracy, error rate, precision, recall, etc.).\n",
        "2. Neural networks are time-consuming for training as fitting model parameters is slow and has many hyperparameters.\n",
        "3. Random forests are an excellent compromise between classification performance and hyperparameter tuning. They can often provide competitive accuracy without requiring much hyperparameter tuning.\n",
        "4. Neural networks are data-hungry and perform poorly in small datasets.\n",
        "5. Decision trees offer model interpretability but are not competitive in accuracy.\n",
        "6. Neural networks are the best models when learning from unstructured data such as images.\n",
        "7. Random forests are the best models when learning from structured data such as a tabular dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCrbkcLxrrj1"
      },
      "source": [
        "## Task 0 - Datasets description, downloading and loading the data into a Pandas dataframe\n",
        "\n",
        "We have selected five publicly available benchmark datasets:\n",
        "\n",
        "1. **UCI adult income dataset.** This is a binary classification dataset in which we want to predict if a person earns more than $50k/year. It is a mid-size dataset (48K examples) with 14 features of mixed data types (categorical and continuous) with missing values.\n",
        "\n",
        "2. **Forest cover type dataset.** This is a large multi-class dataset with 580K examples and 54 features of mixed types. The objective is to predict the type of forest cover based on features such as soil type, elevation, and slope.\n",
        "\n",
        "3. **California housing prices**. This is a regression problem in which we want to predict housing prices based on numerical features, such as population, median income and location. It has 20K instances and nine features.\n",
        "\n",
        "4. **Fashion MNIST dataset**. It is an image classification dataset that is very similar to MNIST. Images are $28 \\times 28$ grayscale pixels. The objective is to classify ten different types of clothing. It has 60k training and 10K test instances.\n",
        "\n",
        "5. **Credit card fraud detection**. This is a binary classification dataset for detecting fraudulent transactions in credit card data. It is highly imbalanced, meaning that most transactions are normal, with some rare fraud cases. It has 284K instances and 30 numerical features.\n",
        "\n",
        "This table summarises the datasets.\n",
        "\n",
        "| Dataset                          | Problem Type        | Feature Type                          | Size        | Notable Challenge                                    |\n",
        "|-----------------------------------|---------------------|---------------------------------------|-------------|------------------------------------------------------|\n",
        "| **UCI Adult Income**              | Binary Classification| Categorical and Numerical             | 48,000      | Mix of feature types with missing values           |\n",
        "| **Forest Cover Type**             | Multi-class Classification | Categorical and Numerical       | 580,000     | Large dataset with mix of feature types      |\n",
        "| **California Housing Prices**     | Regression          | Numerical                              | 20,000      | Regression task      |\n",
        "| **Fashion MNIST**                 | Multi-class Classification (Image)| Image (grayscale)        | 60,000      | Weak features in the form of individual pixels brightness       |\n",
        "| **Credit Card Fraud Detection**   | Binary Classification (Imbalanced)| Numerical                | 284,000     | Highly imbalanced dataset        |\n",
        "\n",
        "Let's start by downloading the data from GitHub. The cell below will download and save the data into a local ``data`` folder. We will use the data later to train and assess our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5hpjwssrrj1",
        "outputId": "4610ecbd-a353-42fb-ae68-df26a30c04d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading adult.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip...\n",
            "Downloading covertype.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip...\n",
            "Downloading california_housing.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip...\n",
            "Downloading creditcard.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip...\n",
            "Downloading fashion_mnist.zip from https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip...\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It downloads and unzips the datasets to your local disk.\n",
        "\n",
        "def download_and_extract(url, extract_to):\n",
        "    \"\"\"\n",
        "    Download a zip file from the URL and extract it to the specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    - url (str): The URL of the zip file to download.\n",
        "    - extract_to (str): The directory where the zip file's contents will be extracted.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Get the file, dataset names from the URL\n",
        "    zip_filename = url.split(\"/\")[-1]\n",
        "    dataset_name = zip_filename.split(\".\")[0]\n",
        "\n",
        "    # Each dataset will have its folder\n",
        "    extract_to = extract_to + \"/\" + dataset_name\n",
        "\n",
        "    # Download the zip file\n",
        "    print(f\"Downloading {zip_filename} from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    with open(zip_filename, \"wb\") as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Create the extraction directory if it doesn't exist\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    # Unzip the file\n",
        "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "    # Remove the zip files\n",
        "    os.remove(zip_filename)\n",
        "\n",
        "# These are the URLs to the datasets. We have hosted the data on GitHub.\n",
        "urls = [\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/adult/adult.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/covertype/covertype.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/california_housing/california_housing.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/creditcard/creditcard.zip\",\n",
        "    \"https://raw.githubusercontent.com/UNSW-COMP9414/Assignment2/main/data/fashion_mnist/fashion_mnist.zip\",\n",
        "]\n",
        "\n",
        "for i, url in enumerate(urls):\n",
        "    download_and_extract(url, \"data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK5bX035rrj1"
      },
      "source": [
        "### Loading data into pandas\n",
        "\n",
        "The datasets are well-diversified in size (number of examples and features), number of class labels, feature types (continuous and discrete), class distribution, and presence of missing data.\n",
        "\n",
        "All datasets have pre-defined training and testing splits. We will use the training set to train the models and choose hyperparameters. You may further split the training set into training and validation sets. The test set should only be used to assess and compare the models.\n",
        "\n",
        "The next cell has a supporting function that loads a specified dataset training and test sets into a pandas' dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbW1wRyirrj2",
        "outputId": "6b884d1d-1c9c-498b-f119-b1fb556f6f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data/adult...\n",
            "Train Features Shape: (32561, 14), Train Labels Shape: (32561, 1)\n",
            "Test Features Shape: (16281, 14), Test Labels Shape: (16281, 1)\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It is a helper function that loads data from this into a Pandas dataframe.\n",
        "\n",
        "def load_train_test_data(path):\n",
        "    \"\"\"\n",
        "    Loads the train and test CSV files and returns them split into features (X) and labels (y).\n",
        "\n",
        "    Parameters:\n",
        "    - path (str): Path to the train and test CSV files.\n",
        "\n",
        "    Returns:\n",
        "    - X_train (DataFrame): Features of the training dataset.\n",
        "    - y_train (DataFrame): Labels of the training dataset.\n",
        "    - X_test (DataFrame): Features of the test dataset.\n",
        "    - y_test (DataFrame): Labels of the test dataset.\n",
        "    \"\"\"\n",
        "    # Load the training and testing data\n",
        "    train_df = pd.read_csv(f\"{path}/train.csv\")\n",
        "    test_df = pd.read_csv(f\"{path}/test.csv\")\n",
        "\n",
        "    # Select class label columns (those starting with 'Target')\n",
        "    y_train = train_df.filter(regex='^Target')\n",
        "    y_test = test_df.filter(regex='^Target')\n",
        "\n",
        "    # Select feature columns (all columns except the ones with 'Target' prefix)\n",
        "    X_train = train_df.drop(columns=y_train.columns)\n",
        "    X_test = test_df.drop(columns=y_test.columns)\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Example usage:\n",
        "path = \"data/adult\"\n",
        "print(f\"Loading {path}...\")\n",
        "X_train, y_train, X_test, y_test = load_train_test_data(path)\n",
        "\n",
        "print(f\"Train Features Shape: {X_train.shape}, Train Labels Shape: {y_train.shape}\")\n",
        "print(f\"Test Features Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ8FTbRbrrj2"
      },
      "source": [
        "## Task 1 [14 Marks] - Data preprocessing\n",
        "\n",
        "Your first task is to preprocess the datasets. Preprocessing usually involves data cleaning and transformation to improve data quality and prepare the data for the specific requirements of the learning approaches.\n",
        "\n",
        "For data preparation, we have the following tasks:\n",
        "1. **Missing imputation (all models)**: The adult dataset has missing values, and none of our learning algorithm implementations can directly handle missing data. Two missing data treatments are eliminating the rows with missing data or replacing the missing values with estimated ones. *Mean imputation*, as the name suggests, replaces missing values with the attribute mean, median (continuous features) or mode (discrete features). These statistics must only be estimated in the training set.\n",
        "2. **Feature encoding (all models)**: Neural networks, tree and random forest implementations available in the Scikit-Learn library do not handle categorical attributes directly. Therefore, these attributes need to be converted into numerical attributes. Although several encoding approaches exist, we will use one-hot encoding, as it is simple and recommended for categorical features with a small cardinality.\n",
        "3. **Class attribute encoding (neural networks only)**: Neural networks also need a one-hot encoding for the class attribute. This step is not necessary for the tree models.\n",
        "4. **Rescaling attribute values (neural networks only)**: The neural network's training benefits from rescaling the attribute values. In this task, we will convert each attribute to a number in the 0 to 1 range by using a simple linear rescaling: $x_s = \\frac{x-min_f}{max_f-min_f}$, where $x_s$ is the recalled $x$ value, $min_f$ is the minimum and $max_f$ the maximum values for feature $f$ in the training data.\n",
        "\n",
        "Tree models typically do not use class encoding and rescaling. The reason is twofold: first, this preprocessing does not help these models fit better parameters; second, tree models are known for their interpretability, and these manipulations create models that are not easier and often harder to understand. Feature encoding is also unnecessary for many tree model implementations, including the well-known [XGBoost](https://xgboost.readthedocs.io/en/stable/) and [LightGBM](https://lightgbm.readthedocs.io/en/stable/). Unfortunately, the Scikit-Learn implementation of tree models does not support categorical attributes.\n",
        "\n",
        "**Warning**: Leaking information from the test set to the training set, even if such information is aggregated data such as means, maximums, and minimums, is considered a serious methodological error. For instance, mean imputation should use the mean only in the training set. Similarly, the maximum and minimum for attribute rescaling should be calculated in the training set. Consequently, we may see values outside the range of 0-1 in the rescaled test set. This mimics the situation in which we find extreme values after the model deployment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrTTZqQerrj2"
      },
      "source": [
        "### Task 1.1 [6 Marks] - Missing data removal or imputation\n",
        "\n",
        "Create a function ``missing_data(X_train, X_test)`` that imputes missing values in the dataframes `X_train` and `X_test`. When the function returns, both dataframes should have no missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "gG578NpArrj2"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "# This cell will be assessed. Replace the ... with your code\n",
        "def missing_data(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Impute missing values in the train and test DataFrames using median/mode imputation.\n",
        "    Missing data statistics are only estimated on the training set and applied to the training and test sets.\n",
        "    Pro-tip: you can use Scikit-Learn's SimpleImputer.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_filled (DataFrame): Training features with no missing values.\n",
        "    - X_test_filled (DataFrame): Test features with no missing values.\n",
        "    \"\"\"\n",
        "    num_cols = X_train.select_dtypes(include=np.number).columns\n",
        "    cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # If there are no categorical or numerical columns, return the original DataFrames\n",
        "    if not num_cols.any() and not cat_cols.any():\n",
        "        return X_train, X_test\n",
        "\n",
        "\n",
        "    # Impute numerical columns with mean\n",
        "    num_imputer = SimpleImputer(strategy='mean')\n",
        "    X_train_num = pd.DataFrame(num_imputer.fit_transform(X_train[num_cols]), columns=num_cols, index=X_train.index)\n",
        "    X_test_num = pd.DataFrame(num_imputer.transform(X_test[num_cols]), columns=num_cols, index=X_test.index)\n",
        "\n",
        "    # Impute categorical columns with most frequent - only if there are categorical columns\n",
        "    if cat_cols.any():\n",
        "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "        X_train_cat = pd.DataFrame(cat_imputer.fit_transform(X_train[cat_cols]), columns=cat_cols, index=X_train.index)\n",
        "        X_test_cat = pd.DataFrame(cat_imputer.transform(X_test[cat_cols]), columns=cat_cols, index=X_test.index)\n",
        "\n",
        "        # Concatenate numerical and categorical imputed data\n",
        "        X_train_imputed = pd.concat([X_train_num, X_train_cat], axis=1)\n",
        "        X_test_imputed = pd.concat([X_test_num, X_test_cat], axis=1)\n",
        "    else:\n",
        "        # If no categorical columns, use only numerical imputed data\n",
        "        X_train_imputed = X_train_num\n",
        "        X_test_imputed = X_test_num\n",
        "\n",
        "\n",
        "    return X_train_imputed, X_test_imputed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMlBcOZ7rrj2"
      },
      "source": [
        "### Task 1.2 [4 Marks] - Feature and class encoding\n",
        "\n",
        "Let's implement a function ``encoding(X_train, X_test)`` that creates one-hot encodings for all categorical attributes. All categorical attributes are encoded as one-hot numeric features when the function returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Oi5mhU9srrj2"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def encoding(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Encodes categorical features and class labels into one-hot numeric features.\n",
        "    Ensure that you have a consistent encoding across training and test sets.\n",
        "    Pro-tip: use Panda's get_dummies.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_encoded (DataFrame): One-hot encoded training features.\n",
        "    - X_test_encoded (DataFrame): One-hot encoded test features.\n",
        "    \"\"\"\n",
        "    col = X_train.select_dtypes(include=['object','category']).columns\n",
        "    combine = pd.concat([X_train, X_test], keys=['train','test'])\n",
        "    combine_encoded = pd.get_dummies(combine, columns=col)\n",
        "    X_train_encoded = combine_encoded.xs('train')\n",
        "    X_test_encoded = combine_encoded.xs('test')\n",
        "\n",
        "\n",
        "    return X_train_encoded, X_test_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfb26Mkvrrj3"
      },
      "source": [
        "#### Task 1.3 [4 Marks] - Rescaling attributes\n",
        "\n",
        "To conclude the pre-processing task, let's create a function ``rescale(X_train, X_test)`` that rescales all continuous attributes so that each attribute is between 0 and 1. When the function returns, all numerical attributes should be rescaled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Su4jS5uxrrj3"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def rescale(X_train, X_test):\n",
        "    \"\"\"\n",
        "    Rescales all continuous attributes in the train and test datasets to be in the range [0, 1].\n",
        "    Rescaling statistics should only be estimated on the training set and applied to the training and test sets.\n",
        "    Pro-tip: use MinMaxScaler.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - X_test (DataFrame): Test features.\n",
        "\n",
        "    Returns:\n",
        "    - X_train_rescaled (DataFrame): Rescaled training features.\n",
        "    - X_test_rescaled (DataFrame): Rescaled test features.\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    col = X_train.select_dtypes(include=['number']).columns\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(X_train[col])\n",
        "    X_train_rescaled = X_train.copy()\n",
        "    X_test_rescaled = X_test.copy()\n",
        "    X_train_rescaled[col] = scaler.transform(X_train[col])\n",
        "    X_test_rescaled[col] = scaler.transform(X_test[col])\n",
        "\n",
        "    return X_train_rescaled, X_test_rescaled\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbDPF5sJrrj3"
      },
      "source": [
        "### Preprocessing the datasets\n",
        "\n",
        "In the cell below, we will call your functions to preprocess the datasets. We will create two versions of each dataset: the first is suitable for the tree models and will have no missing values and encoded attributes. The second will have no missing values, encoded categorical and class features, and numeric features rescaled. We will save these datasets for use later. The datasets pre-processed for trees will be saved in a ``tree`` folder. The datasets for neural networks will be saved in a ``nn`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qrx60sTYrrj3",
        "outputId": "c4593392-969a-4f47-bc11-dc0394d84610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Processing dataset: covertype\n",
            "Processing dataset: california_housing\n",
            "Processing dataset: fashion_mnist\n",
            "Processing dataset: creditcard\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It calls your pre-processing functions and saves the preprocessed datasets on disk.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"california_housing\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(\"data/\" + dataset)\n",
        "\n",
        "    # Preprocessing for tree-based models\n",
        "    tree_path = f\"data/{dataset}/tree\"\n",
        "    if not os.path.exists(tree_path):\n",
        "        os.makedirs(tree_path)\n",
        "\n",
        "    # Handle missing data\n",
        "    X_train, X_test = missing_data(X_train, X_test)\n",
        "\n",
        "    # Apply encoding features\n",
        "    X_train_encoded, X_test_encoded = encoding(X_train, X_test)\n",
        "\n",
        "    # Concatenate X and y for train and test data.\n",
        "    # For decision trees, we do not encode the class attribute\n",
        "    train_tree = pd.concat([X_train_encoded, y_train.reset_index(drop=True)], axis=1)\n",
        "    test_tree = pd.concat([X_test_encoded, y_test.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save tree-preprocessed datasets\n",
        "    train_tree.to_csv(f\"{tree_path}/train.csv\", index=False)\n",
        "    test_tree.to_csv(f\"{tree_path}/test.csv\", index=False)\n",
        "\n",
        "    # Preprocessing for neural networks\n",
        "    nn_path = f\"data/{dataset}/nn\"\n",
        "    if not os.path.exists(nn_path):\n",
        "        os.makedirs(nn_path)\n",
        "\n",
        "    # Apply encoding class attribute. For a regression dataset, the next line should do nothing\n",
        "    y_train_encoded, y_test_encoded = encoding(y_train, y_test)\n",
        "\n",
        "    # Rescale the features\n",
        "    X_train_rescaled, X_test_rescaled = rescale(X_train_encoded, X_test_encoded)\n",
        "\n",
        "    # Concatenate X and y for train and test data\n",
        "    train_nn = pd.concat([X_train_rescaled, y_train_encoded.reset_index(drop=True)], axis=1)\n",
        "    test_nn = pd.concat([X_test_rescaled, y_test_encoded.reset_index(drop=True)], axis=1)\n",
        "\n",
        "    # Save nn-preprocessed datasets\n",
        "    train_nn.to_csv(f\"{nn_path}/train.csv\", index=False)\n",
        "    test_nn.to_csv(f\"{nn_path}/test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urYwTbXnrrj3"
      },
      "source": [
        "## Task 2 - [16 Marks] Model Training\n",
        "\n",
        "We have the data ready, and in this task, we will train some initial models for each dataset. We will refine the models later, but for now, we will create a swallow model for the neural network. The decision tree and the random forest models will use Scikit-Learn's default hyperparameter values for these models.\n",
        "\n",
        "The neural network will have three layers: the input layer ($i$), one hidden layer ($h$) and one output layer ($o$). We will use a simple rule-of-thumb for the number of units in the hidden layer: $D_h = \\sqrt{D_i * D_o}$. The other hyperparameters are similar to the ones used in the Week 07 tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muen63VJrrj3"
      },
      "source": [
        "### Task 2.1 [4 Marks] - Shallow neural net for classification\n",
        "\n",
        "Create a function ``train_shallow_net_class(X_train, y_train)`` that trains a shallow neural net for classification using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and softmax on the output layer.\n",
        "3. Categorical cross-entropy as loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sOv2Ze-wrrj3"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_shallow_net_class(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a shallow neural net for classification problems with one hidden layer using the training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training labels (one-hot encoded for classification).\n",
        "    - Dh (int): Number of units in the hidden layer.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras neural network model.\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    import math\n",
        "    Di = X_train.shape[1]\n",
        "    Do = y_train.shape[1]\n",
        "    Dh = round(math.sqrt(Di * Do))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(Di,)))  # 使用 Input 层定义输入形状\n",
        "    model.add(Dense(Dh, activation='relu'))\n",
        "    model.add(Dense(Do, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2, verbose=0)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUk2aY2rrj3"
      },
      "source": [
        "The next cell will call your function to train a shallow model for each classification dataset, compute the training time, and test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYi2wWUxrrj3",
        "outputId": "7cce000a-046b-462f-f107-8f5a17f76824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "\u001b[1m509/509\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.3236\n",
            "Test error rate: 0.1503\n",
            "Runtime to train the model: 38.467936515808105 seconds\n",
            "Processing dataset: covertype\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the classification datasets.\n",
        "\n",
        "results = defaultdict(dict)\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_class(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] =  1 - test_accuracy         # Error rate = 1 - accuracy\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjm4jiPjrrj3"
      },
      "source": [
        "### Task 2.2 [4 Marks] - Shallow neural net for regression\n",
        "\n",
        "Like the previous task, create a function ``train_shallow_net_regression(X_train, y_train)`` that trains a shallow neural net for regression using the training data ``X_train`` and labels ``y_train``. Use the following hyperparameters:\n",
        "1. A single hidden layer with $D_h = \\text{round}(\\sqrt{D_i * D_o})$ units.\n",
        "2. ReLU activation in the hidden layer and linear on the output layer.\n",
        "3. MSE loss function.\n",
        "4. Train for 30 epochs.\n",
        "5. Batch size of 32 instances.\n",
        "6. Validation split of 20% of the training data.\n",
        "7. Adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "CvxkmyfVrrj3"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_shallow_net_regression(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a shallow neural net with one hidden layer using the training data.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training labels (one-hot encoded for classification).\n",
        "    - Dh (int): Number of units in the hidden layer.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras neural network model.\n",
        "    \"\"\"\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    import math\n",
        "\n",
        "    Di = X_train.shape[1]\n",
        "    Do = 1\n",
        "    Dh = round(math.sqrt(Di * Do))\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(Di,)))\n",
        "    model.add(Dense(Dh, activation='relu'))\n",
        "    model.add(Dense(Do, activation='linear'))\n",
        "    model.compile(optimizer='adam', loss='mean_squred_error',metrics=['mse'])\n",
        "    model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2,verbose=0)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9-NuCK_rrj3"
      },
      "source": [
        "Once again, we will run your code for each regression dataset. This assignment has only one of such datasets, but we will keep a similar code we implemented before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mP37mXu2rrj3",
        "outputId": "85cb9a82-91ee-4b71-b801-92210d022541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.5419 - mse: 0.5419\n",
            "Test MSE: 0.5377\n",
            "Runtime to train the model: 16.87509298324585 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your shallow model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_shallow_net_regression(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTOCbbH9rrj4"
      },
      "source": [
        "### Task 2.3 [2 Marks] - Decision tree models for classification\n",
        "\n",
        "Implement a function ``train_classification_tree(X_train, y_train)`` that trains a decision tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8aeDVkzqrrj4"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_classification_tree(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree for classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training class labels.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree Classifier.\n",
        "    \"\"\"\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wijWIMbrrj4"
      },
      "source": [
        "The code below executes the tree models and records the test accuracy and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdXBFDG-rrj4",
        "outputId": "496c9c67-050e-4be7-b7db-a2d68d3ae1ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training decision tree model\n",
            "Test error rate: 0.1871\n",
            "Runtime to train the model: 0.39745259284973145 seconds\n",
            "Processing dataset: covertype\n",
            "Training decision tree model\n",
            "Test error rate: 0.0945\n",
            "Runtime to train the model: 7.048149824142456 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training decision tree model\n",
            "Test error rate: 0.2103\n",
            "Runtime to train the model: 45.26796245574951 seconds\n",
            "Processing dataset: creditcard\n",
            "Training decision tree model\n",
            "Test error rate: 0.0009\n",
            "Runtime to train the model: 21.438186168670654 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqPk6EbMrrj4"
      },
      "source": [
        "### Task 2.4 [2 Marks] - Decision tree models for regression\n",
        "\n",
        "Implement a function ``train_regression_tree(X_train, y_train)`` that trains a regression tree model using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn decision tree regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KLZgicvkrrj4"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_regression_tree(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Decision Tree for regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree Regressor.\n",
        "    \"\"\"\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    model = DecisionTreeRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uyLcJa6rrj4"
      },
      "source": [
        "The code below executes the regression tree models and saves the running time and test mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUQFjs8wrrj4",
        "outputId": "a9e264e0-e7ea-41b5-f24a-08cbf9088791"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training regression tree model\n",
            "Test accuracy: 0.5100\n",
            "Runtime to train the model: 0.18619227409362793 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression tree model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training regression tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_tree(X_train, y_train)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test accuracy: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYyUZIIurrj4"
      },
      "source": [
        "### Task 2.5 [2 Marks] - Random forest models for classification\n",
        "\n",
        "Implement a function ``train_classification_forest(X_train, y_train)`` that trains a random forest model for classification using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ppSbf4xXrrj4"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_classification_forest(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest for classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training class labels.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest Classifier.\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpvUEP0rrj4"
      },
      "source": [
        "The code below executes the randon forest models and records the training time and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPqbg7jirrj8",
        "outputId": "ea6aae89-b482-4b36-be2d-8eb924523fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "Training random forest model\n",
            "Test error rate: 0.1491\n",
            "Runtime to train the model: 4.520358085632324 seconds\n",
            "Processing dataset: covertype\n",
            "Training random forest model\n",
            "Test error rate: 0.0765\n",
            "Runtime to train the model: 92.6226966381073 seconds\n",
            "Processing dataset: fashion_mnist\n",
            "Training random forest model\n",
            "Test error rate: 0.1249\n",
            "Runtime to train the model: 97.11938285827637 seconds\n",
            "Processing dataset: creditcard\n",
            "Training random forest model\n",
            "Test error rate: 0.0004\n",
            "Runtime to train the model: 189.38865733146667 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_classification_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Test error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY6QT4gvrrj8"
      },
      "source": [
        "### Task 2.6 [2 Marks] - Random Forest Models for Regression\n",
        "\n",
        "Finally, implement a function ``train_regression_forest(X_train, y_train)`` that trains a random forest model for regression using the training data ``X_train`` and labels ``y_train``. This function should return a trained Scikit-Learn random forest regressor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QDoPyVfZrrj8"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def train_regression_forest(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Trains a Random Forest for regression.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest Regressor.\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    model = RandomForestRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0bRnPEfrrj8"
      },
      "source": [
        "The code below executes the random forest models for regression and records the training time and mean squared error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA3gcgOarrj8",
        "outputId": "b222eddd-13f6-45c7-d7a2-846876faf5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: california_housing\n",
            "Training random forest model\n",
            "Test MSE: 0.2612\n",
            "Runtime to train the model: 12.388702869415283 seconds\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = train_regression_forest(X_train, np.array(y_train).ravel())\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to train the model: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEWLrCphrrj8"
      },
      "source": [
        "### Summarising the Results\n",
        "\n",
        "Congratulations, we have reached the end of Task 2. The next cell will summarise the results obtained in a single table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmgoDe4arrj8",
        "outputId": "5175ed7e-7c85-46e5-927e-4019f4998c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+--------------------+---------------+\n",
            "|      Dataset       |     Model     | Prediction quality | Training time |\n",
            "+--------------------+---------------+--------------------+---------------+\n",
            "|       adult        |  Neural Net   |       0.1512       |    39.9210    |\n",
            "|       adult        | Decision Tree |       0.1871       |    0.3975     |\n",
            "|       adult        | Random Forest |       0.1491       |    4.5204     |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "| california_housing |  Neural Net   |       0.5377       |    16.8751    |\n",
            "| california_housing | Decision Tree |       0.5100       |    0.1862     |\n",
            "| california_housing | Random Forest |       0.2612       |    12.3887    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     covertype      |  Neural Net   |       0.3111       |   411.0693    |\n",
            "|     covertype      | Decision Tree |       0.0945       |    7.0481     |\n",
            "|     covertype      | Random Forest |       0.0765       |    92.6227    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|     creditcard     |  Neural Net   |       0.0006       |   207.3574    |\n",
            "|     creditcard     | Decision Tree |       0.0009       |    21.4382    |\n",
            "|     creditcard     | Random Forest |       0.0004       |   189.3887    |\n",
            "|        ----        |     ----      |        ----        |     ----      |\n",
            "|   fashion_mnist    |  Neural Net   |       0.1170       |    69.4915    |\n",
            "|   fashion_mnist    | Decision Tree |       0.2103       |    45.2680    |\n",
            "|   fashion_mnist    | Random Forest |       0.1249       |    97.1194    |\n",
            "+--------------------+---------------+--------------------+---------------+\n"
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It summarises the results in a tabular format\n",
        "\n",
        "def format_values(row):\n",
        "    \"\"\"Format numeric values to 4 decimal places.\"\"\"\n",
        "    return {k: f\"{v:.4f}\" if isinstance(v, float) else v for k, v in row.items()}\n",
        "\n",
        "def print_results_table(results):\n",
        "    \"\"\"\n",
        "    Converts a nested dictionary of results into a table and prints it with separation lines for datasets.\n",
        "    \"\"\"\n",
        "    # Flatten the nested dictionary into a list of rows\n",
        "    flattened_data = [\n",
        "        {\"Dataset\": dataset, \"Model\": model, **metrics}\n",
        "        for model, datasets in results.items()\n",
        "        for dataset, metrics in datasets.items()\n",
        "    ]\n",
        "\n",
        "    # Sort the data by the \"Dataset\" column\n",
        "    flattened_data_sorted = sorted(flattened_data, key=lambda x: x[\"Dataset\"])\n",
        "\n",
        "    # Add separator rows between datasets\n",
        "    formatted_data = []\n",
        "    previous_dataset = None\n",
        "    for row in flattened_data_sorted:\n",
        "        if previous_dataset and row[\"Dataset\"] != previous_dataset:\n",
        "            # Insert a separator row\n",
        "            formatted_data.append({key: \"----\" for key in row.keys()})\n",
        "        formatted_data.append(format_values(row))\n",
        "        previous_dataset = row[\"Dataset\"]\n",
        "\n",
        "    # Extract headers\n",
        "    headers = list(formatted_data[0].keys())\n",
        "\n",
        "    # Generate and print the table\n",
        "    table = tabulate(formatted_data, headers=\"keys\", tablefmt=\"pretty\", missingval=\"N/A\")\n",
        "    print(table)\n",
        "\n",
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbuGVQC_rrj8"
      },
      "source": [
        "## Task 3 [32 Marks] - Hyperparameter optimisation\n",
        "\n",
        "So far, we have used a fixed set of hyperparameters, but it is unclear if they are a good choice for our datasets. We will use Keras Tuner and Scikit Learn libraries to test different hyperparameter combinations. We will start with the Neural Net models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zazbTTGZrrj8"
      },
      "source": [
        "### Task 3.1 [8 Marks] - Hyperparameter optimisation for classification neural nets\n",
        "\n",
        "Create a function ``tune_train_classification_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and labels. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. Here are some suggestions based on the tutorials:\n",
        "1. Depth. To make your model deeper, test a larger number of hidden layers, up to 3.\n",
        "2. Width. Try different combinations of numbers of neurons per layer. For instance, you can try from $D_h / 2$ to $D_h * 2$.\n",
        "3. Activation functions. ReLU, TANH and Sigmoid are common choices.\n",
        "4. Optimiser. Adam and SGD.\n",
        "5. Learning rate. A typical range is 1e-4 to 1e-2.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lPpiiSlNrrj9"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_classification_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"\n",
        "    Tunes and trains a classification neural network using Random Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (one-hot or integer labels).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try.\n",
        "    - project_name (str): Name for organizing logs and results.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras model with the best hyperparameters.\n",
        "    \"\"\"\n",
        "    from keras_tuner import HyperModel\n",
        "    from keras_tuner.tuners import RandomSearch\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    from tensorflow.keras.optimizers import Adam, SGD\n",
        "    import math\n",
        "\n",
        "    Di = X_train.shape[1]\n",
        "    Do = y_train.nunique() if y_train.ndim == 1 else y_train.shape[1]\n",
        "    Dh = round(math.sqrt(Di * Do))\n",
        "\n",
        "    class MyHyperModel(HyperModel):\n",
        "      def build(self, hp):\n",
        "        model = Sequential()\n",
        "        for i in range(hp.Int('num_layers', 1, 3)):\n",
        "          units = hp.Int('units_' + str(i), min_value=Dh // 2, max_value=Dh * 2, step=16)\n",
        "          activation = hp.Choice('activation_' + str(i), values=['relu', 'tanh', 'sigmoid'])\n",
        "          if i == 0:\n",
        "            model.add(Dense(units=units, activation=activation, input_shape=(Di,)))\n",
        "          else:\n",
        "            model.add(Dense(units=units, activation=activation))\n",
        "        model.add(Dense(units=Do, activation='softmax'))\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
        "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "        if optimizer == 'adam':\n",
        "          optimizer = Adam(learning_rate=learning_rate)\n",
        "        else:\n",
        "          optimizer = SGD(learning_rate=learning_rate)\n",
        "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "    tuner = RandomSearch(\n",
        "        MyHyperModel(),\n",
        "        objective='val_accuracy',\n",
        "        max_trials=n_iter,\n",
        "        executions_per_trial=3,  # Average performance over three runs\n",
        "        directory='hyperparameter_search',\n",
        "        project_name=project_name\n",
        "    )\n",
        "    tuner.search(X_train,y_train,epochs=30,batch_size=32,validation_split=0.2,verbose=0)\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    return best_model\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdXa1kDTrrj9"
      },
      "source": [
        "#### Important notice about the runtime\n",
        "\n",
        "Hyperparameter tuning can take a lot of time, as most Machine Learning algorithms have many hyperparameters, and testing all possible combinations can lead to a combinatorial explosion.\n",
        "\n",
        "To make comparisons fairer, we will limit the hyperparameter search to using no more than **approximately** 30 minutes of computing time.\n",
        "\n",
        "The table above tells us the training time for a single model. For instance, if a random forest takes 4s for the adult dataset, then in 1,800 seconds (30 minutes), we can train 1,800 / 4 = 450 models. Each hyperparameter combination performance will be an average of three repetitions. Thus, we can assess 450 / 3 = 150 hyperparameter combinations.\n",
        "\n",
        "We will control the time using the ``n_iter`` parameter. This parameter defines the maximum number of parameter combinations sampled and tested during the search. Given their smaller number of hyperparameters, some inducers, particularly the trees, may run much faster than 30 minutes.\n",
        "\n",
        "This is a rough approximation based on a single run of the default models. Thus, some models may run faster or slower than 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6JnlQL7crrj9",
        "outputId": "0dc9d510-9825-4c8c-be66-a61088676467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing dataset: adult\n",
            "\tTuning and training neural net model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-491ce694943c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtune_train_classification_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_in_seconds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Neural Net\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Training time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-693bfabeb8ba>\u001b[0m in \u001b[0;36mtune_train_classification_net\u001b[0;34m(X_train, y_train, n_iter, project_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     )\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your optimised neural net model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"\\tTuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D13MSbACrrj9"
      },
      "source": [
        "### Task 3.2 [8 Marks] - Hyperparameter optimisation for regression Neural Nets\n",
        "\n",
        "Create a function ``tune_train_regression_net(X_train, y_train, n_iter, project_name)`` that uses Keras tuner's ``RandomSearch`` to optimise the hyperparameters of a regression neural net model. ``X_train`` and ``y_train`` are pandas dataframes with the training data and target values. ``n_iter`` is the maximum number of iterations in the random search. ``project_name`` is an identifier used by Keras tuner to save the results on disk.\n",
        "\n",
        "You have the freedom to choose your hyperparameter search space. You can use the same hyperparameter recommendations given for classification.\n",
        "\n",
        "Your function should return the Keras model that achieved the best performance in a validation set of 20% of the training data. Average performance over three runs (``executions_per_trial=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdgLND6Wrrj9"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_regression_net(X_train, y_train, n_iter, project_name):\n",
        "    \"\"\"\n",
        "    Tunes and trains a regression neural network using Random Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values.\n",
        "    - n_iter (int): Number of hyperparameter configurations to try.\n",
        "    - project_name (str): Name for organising logs and results.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Keras model with the best hyperparameters.\n",
        "    \"\"\"\n",
        "    import keras_tuner as kt\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense\n",
        "    from tensorflow.keras.optimizers import Adam, SGD\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    import math\n",
        "\n",
        "    Di = X_train.shape[1]\n",
        "    Dh = round(math.sqrt(Di))  # Width (Dh) for hidden layers based on input dimensions\n",
        "\n",
        "    # Define the hypermodel\n",
        "    def build_model(hp):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Adding hidden layers with tunable depth and width\n",
        "        for i in range(hp.Int('num_layers', 1, 3)):  # Up to 3 hidden layers\n",
        "            units = hp.Int('units_' + str(i), min_value=Dh // 2, max_value=Dh * 2, step=16)\n",
        "            activation = hp.Choice('activation_' + str(i), values=['relu', 'tanh', 'sigmoid'])\n",
        "            if i == 0:\n",
        "                model.add(Dense(units=units, activation=activation, input_shape=(Di,)))\n",
        "            else:\n",
        "                model.add(Dense(units=units, activation=activation))\n",
        "\n",
        "        # Output layer with a single neuron for regression\n",
        "        model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "        # Compile the model with a tunable optimizer and learning rate\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'sgd'])\n",
        "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        else:\n",
        "            optimizer = SGD(learning_rate=learning_rate)\n",
        "\n",
        "        model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "        return model\n",
        "\n",
        "    X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_model,\n",
        "        objective='val_mae',\n",
        "        max_trials=n_iter,\n",
        "        executions_per_trial=3,\n",
        "        directory='hyperparameter_search',\n",
        "        project_name=project_name\n",
        "    )\n",
        "\n",
        "    tuner.search(X_train_split, y_train_split, validation_data=(X_val, y_val), epochs=10, verbose=1)\n",
        "\n",
        "    best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU4dasUXrrj9"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your neural net model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/nn/\")\n",
        "\n",
        "    print(\"Tuning and training neural net model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_net(X_train, y_train, int(timeout_in_seconds / 3 / results[\"Neural Net\"][dataset][\"Training time\"]), dataset)\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    test_loss, test_mse = model.evaluate(X_test, y_test)\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Neural Net (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Neural Net (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpyhP15Crrj9"
      },
      "source": [
        "### Task 3.3 [4 Marks] - Hyperparameter optimisation for decision trees\n",
        "\n",
        "We will train the decision trees with hyperparameter optimisation. Our code will implement the search using the ``RandomizedSearchCV`` class.\n",
        "\n",
        "We will create the function ``tune_train_classification_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such a combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANYfV2cWrrj9"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_classification_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Decision Tree for classification using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (class labels).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Decision Tree classifier with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    param_dist = {\n",
        "        'max_depth': [10,20,30,40,None],\n",
        "        'min_samples_split': [2,5,10,20],\n",
        "        'min_samples_leaf': [1,2,5,10],\n",
        "        'criterion': ['gini','entropy']\n",
        "    }\n",
        "    tree = DecisionTreeClassifier(random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=tree, param_distributions=param_dist, n_iter=n_iter, cv=3, scoring='accuracy', random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urn727Fcrrj9"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your decision tree model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training decision tree model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SDETOZZrrj9"
      },
      "source": [
        "### Task 3.4 [4 Marks] - Hyperparameter optimisation for regression trees\n",
        "\n",
        "We will train the regression trees with hyperparameter optimisation through the function ``tune_train_regression_tree(X_train, y_train, n_iter)``, which optimises hyperparameters and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You can use the same suggestion for the hyperparameter space provided in the previous task. However, the splitting criteria suitable for regression trees are different. We suggest ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKzv49o4rrj9"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_regression_tree(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Regression Tree using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (continuous values).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Regression Tree with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    from sklearn.tree import DecisionTreeRegressor\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    param_dist = {\n",
        "        'max_depth': [10,20,30,40,None],\n",
        "        'min_samples_split': [2,5,10,20],\n",
        "        'min_samples_leaf': [1,2,5,10],\n",
        "        'criterion': ['squared_error','friedman_mse','absolute_error']\n",
        "    }\n",
        "    tree = DecisionTreeRegressor(random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=tree, param_distributions=param_dist, n_iter=n_iter, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLM_GKG9rrj9"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your regression model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_tree(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Decision Tree\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Decision Tree (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Decision Tree (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tH1snRGrrj9"
      },
      "source": [
        "### Task 3.5 [4 Marks] - Hyperparameter optimisation for decision forest\n",
        "\n",
        "We will create the function ``tune_train_classification_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a classification random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Here are some suggestions:\n",
        "- Number of estimators (trees): 50, 100, 200.\n",
        "- Maximum tree depth from 10 to 40 with increments of 10. Include None, too.\n",
        "- Minimum samples in a split: 2, 5, 10, 20.\n",
        "- Minimum samples in a leaf node: 1, 2, 5, and 10.\n",
        "- Splitting criteria: gine and entropy.\n",
        "- Bootstrap sampling: yes and no.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PkWvfTDrrj9"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_classification_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Random Forest classifier using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (class labels).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest classifier with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    param_dist = {\n",
        "        'n_estimators': [50,100,200],\n",
        "        'max_depth': [10,20,30,40,None],\n",
        "        'min_samples_split': [2,5,10,20],\n",
        "        'min_samples_leaf': [1,2,5,10],\n",
        "        'criterion': ['gini','entropy'],\n",
        "        'bootstrap': [True,False]\n",
        "    }\n",
        "    forest = RandomForestClassifier(random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=forest, param_distributions=param_dist, n_iter=n_iter, cv=3, scoring='accuracy', random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgmWJv4Arrj-"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the classification datasets.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"adult\", \"covertype\", \"fashion_mnist\", \"creditcard\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"\\tTuning and training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_classification_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'\\t\\tTest error rate: {1 - test_accuracy:.4f}')\n",
        "    print(f'\\t\\tRuntime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = 1 - test_accuracy\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VXpggKxrrj-"
      },
      "source": [
        "### Task 3.6 [4 Marks] - Hyperparameter optimisation for regression forest\n",
        "\n",
        "We will create the function ``tune_train_regression_forest(X_train, y_train, n_iter)``, which optimises hyperparameters for a regression random forest and returns a scikit-learn model trained with the best parameters.\n",
        "\n",
        "You have the freedom to define your hyperparameter search space. Our recommendations are similar for the classification forest. However, the splitting criteria suitable for regression problems are ``squared_error``, ``friedman_mse``, and ``absolute_error``.\n",
        "\n",
        "The function will search for the best combination of hyperparameter values and return a model trained in such combination in the complete training set. During the search, average the performance using 3-fold cross-validation (``cv=3``)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgwXXJ8brrj-"
      },
      "outputs": [],
      "source": [
        "# This cell will be assessed. Replace the ... with your code\n",
        "\n",
        "def tune_train_regression_forest(X_train, y_train, n_iter):\n",
        "    \"\"\"\n",
        "    Tunes and trains a Random Forest regressor using Randomized Search.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (DataFrame): Training features.\n",
        "    - y_train (DataFrame): Training target values (continuous values).\n",
        "    - n_iter (int): Number of hyperparameter configurations to try during the search.\n",
        "\n",
        "    Returns:\n",
        "    - model: Trained Random Forest regressor with the best-found hyperparameters.\n",
        "    \"\"\"\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    param_dist = {\n",
        "        'n_estimators': [50,100,200],\n",
        "        'max_depth': [10,20,30,40,None],\n",
        "        'min_samples_split': [2,5,10,20],\n",
        "        'min_samples_leaf': [1,2,5,10],\n",
        "        'criterion': ['squared_error','friedman_mse','absolute_error'],\n",
        "        'bootstrap': [True,False]\n",
        "    }\n",
        "    forest = RandomForestRegressor(random_state=42)\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=forest, param_distributions=param_dist, n_iter=n_iter, cv=3, scoring='neg_mean_squared_error', random_state=42)\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBqgOjxWrrj-"
      },
      "outputs": [],
      "source": [
        "# Do not change the code in this cell.\n",
        "# This cell has no code to write. It trains and assesses your random forest model using the regression dataset.\n",
        "\n",
        "timeout_in_seconds = 1800\n",
        "datasets = [\"california_housing\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Processing dataset: {dataset}\")\n",
        "\n",
        "    # Load the train and test data\n",
        "    X_train, y_train, X_test, y_test = load_train_test_data(f\"data/{dataset}/tree/\")\n",
        "\n",
        "    print(\"Training random forest model\")\n",
        "\n",
        "    start = time.time()\n",
        "    model = tune_train_regression_forest(X_train, np.array(y_train).ravel(), int(timeout_in_seconds / 3 / results[\"Random Forest\"][dataset][\"Training time\"]))\n",
        "    end = time.time()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    test_mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Runtime to hyperparameter search and model training: {end-start} seconds')\n",
        "\n",
        "    results[\"Random Forest (HO)\"].setdefault(dataset, {})\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Prediction quality\"] = test_mse\n",
        "    results[\"Random Forest (HO)\"][dataset][\"Training time\"] = end-start"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pib6Amqrrj-"
      },
      "source": [
        "The next cell tabulates all the results. HO stands for Hyperparameter Optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2e6PFZLrrj-"
      },
      "outputs": [],
      "source": [
        "print_results_table(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VCd2UFDrrj-"
      },
      "source": [
        "Congratulations! You have reached the end of the assignment. In the remaining of this document, you will analyse the results in a report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAJzWaxBrrj-"
      },
      "source": [
        "## Task 4 [13 Marks] - Report\n",
        "\n",
        "Write a report with less than 1,000 words (around two pages) in the following cells using markdown. You can include graphs and tables in your report. Answer the following questions in your report.\n",
        "\n",
        "- [3 Marks] Discuss the performance of the algorithms in terms of prediction quality and training time. Use plots to compare these methods. Is there a method that stands out?\n",
        "- [3 Marks] Do you think any of the seven hypotheses (machine learning wisdom and misconceptions) presented at the beginning of this assignment are correct? Have you observed any evidence that supports them?\n",
        "- [3 Marks] Is the hyperparameter optimisation worth the time spent? Did you observe significant improvements in prediction quality?\n",
        "- [2 Marks] We have measured the training time of these models, but another important aspect is the inference time. Would you expect some models to be more efficient than others for inference? What is the importance of having efficient models for inference? What is the importance of having efficient models for training?\n",
        "- [2 Marks] The credit card dataset is imbalanced; in this situation, the error rate tends to be very small and difficult to interpret. Extend the performance analysis in this dataset to include the confusion matrix and F1 score. Analyse the performance of the classifiers under these performance measures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aix_IwUnrrj-"
      },
      "source": [
        "Use one or more cell here to write your report."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}